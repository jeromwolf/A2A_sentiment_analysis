"""
LLM Manager - ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ
"""
import os
import logging
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod
from dotenv import load_dotenv

# Gemini
try:
    import google.generativeai as genai
except ImportError:
    genai = None

# OpenAI
try:
    import openai
except ImportError:
    openai = None

# Ollama (Gemma3 ë“± ë¡œì»¬ ëª¨ë¸ìš©)
try:
    import ollama
except ImportError:
    ollama = None

load_dotenv(override=True)
logger = logging.getLogger(__name__)


class LLMProvider(ABC):
    """LLM ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        pass


class GeminiProvider(LLMProvider):
    """Google Gemini ì œê³µì"""
    
    def __init__(self, api_key: str):
        if not genai:
            raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
    async def generate(self, prompt: str, **kwargs) -> str:
        import asyncio
        from google.api_core import exceptions
        
        max_retries = 3
        retry_delay = 10  # seconds
        
        for attempt in range(max_retries):
            try:
                # generate_content is synchronous, so we run it in a thread
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(None, self.model.generate_content, prompt)
                return response.text
            except exceptions.ResourceExhausted as e:
                logger.error(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"{retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # exponential backoff
                else:
                    # ìµœì¢… ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
                    logger.error("Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
                    return '{"summary": "API í• ë‹¹ëŸ‰ ì´ˆê³¼", "score": 0.0}'
            except Exception as e:
                logger.error(f"Gemini ìƒì„± ì˜¤ë¥˜: {e}")
                raise
    
    def is_available(self) -> bool:
        return genai is not None and os.getenv("GEMINI_API_KEY")


class Gemma3Provider(LLMProvider):
    """Gemma3 ë¡œì»¬ ëª¨ë¸ ì œê³µì (Ollama ì‚¬ìš©)"""
    
    def __init__(self, model_name: str = "gemma3:latest"):
        if not ollama:
            raise ImportError("ollama íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ollama")
        
        self.model_name = model_name
        self.client = ollama.Client()
        
    async def generate(self, prompt: str, **kwargs) -> str:
        import asyncio
        try:
            # ollama client.generate is synchronous, so run in executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.client.generate(model=self.model_name, prompt=prompt)
            )
            return response['response']
        except Exception as e:
            logger.error(f"Gemma3 ìƒì„± ì˜¤ë¥˜: {e}")
            raise
    
    def is_available(self) -> bool:
        if not ollama:
            return False
        
        try:
            # Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ê³  ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
            models = self.client.list()
            return any(self.model_name in model['name'] for model in models['models'])
        except:
            return False


class OpenAIProvider(LLMProvider):
    """OpenAI GPT ì œê³µì"""
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):
        if not openai:
            raise ImportError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        
    async def generate(self, prompt: str, **kwargs) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"OpenAI ìƒì„± ì˜¤ë¥˜: {e}")
            raise
    
    def is_available(self) -> bool:
        return openai is not None and os.getenv("OPENAI_API_KEY")


class LLMManager:
    """LLM ê´€ë¦¬ì - ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì œê³µì ì„ íƒ"""
    
    def __init__(self):
        self.provider_name = os.getenv("LLM_PROVIDER", "gemini").lower()
        self.provider = self._create_provider()
        
    def _create_provider(self) -> LLMProvider:
        """ì„¤ì •ëœ ì œê³µì ìƒì„±"""
        logger.info(f"ğŸ¤– LLM ì œê³µì ì´ˆê¸°í™”: {self.provider_name}")
        
        if self.provider_name == "gemini":
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return GeminiProvider(api_key)
            
        elif self.provider_name == "gemma3":
            return Gemma3Provider()
            
        elif self.provider_name == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return OpenAIProvider(api_key)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.provider_name}")
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        return await self.provider.generate(prompt, **kwargs)
    
    def is_available(self) -> bool:
        """ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.provider.is_available()
    
    def get_provider_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ì œê³µì ì •ë³´"""
        return {
            "provider": self.provider_name,
            "available": self.is_available(),
            "class": self.provider.__class__.__name__
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_manager = None

def get_llm_manager() -> LLMManager:
    """LLM ê´€ë¦¬ì ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager
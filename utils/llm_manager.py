"""
LLM Manager - ë‹¤ì¤‘ LLM í”„ë¡œë°”ì´ë” ê´€ë¦¬ ë° ìë™ í´ë°±
í† í°/ì¿¼í„° ê´€ë¦¬, ë¹„ìš© ìµœì í™”, ì‘ì—…ë³„ ëª¨ë¸ ì„ íƒ
"""
import os
import asyncio
import logging
from typing import Dict, List, Optional, Any, Literal
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
import json
import httpx
from dotenv import load_dotenv

# Gemini
try:
    import google.generativeai as genai
except ImportError:
    genai = None

# OpenAI
try:
    import openai
except ImportError:
    openai = None

# Anthropic
try:
    from anthropic import AsyncAnthropic
except ImportError:
    AsyncAnthropic = None

# Ollama (Gemma3 ë“± ë¡œì»¬ ëª¨ë¸ìš©)
try:
    import ollama
except ImportError:
    ollama = None

from utils.config_manager import config
from utils.errors import APIError, APIRateLimitError

load_dotenv(override=True)
logger = logging.getLogger(__name__)

# ì‘ì—… ë³µì¡ë„ íƒ€ì…
ComplexityLevel = Literal["light", "medium", "heavy"]
TaskType = Literal["sentiment", "summary", "financial_analysis", "report_generation", "general"]


class UsageTracker:
    """API ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°"""
    def __init__(self):
        self.usage = {}
        
    def track(self, provider: str, tokens: int, cost: float = 0.0):
        if provider not in self.usage:
            self.usage[provider] = {"tokens": 0, "requests": 0, "cost": 0.0}
        self.usage[provider]["tokens"] += tokens
        self.usage[provider]["requests"] += 1
        self.usage[provider]["cost"] += cost
        
    def get_usage(self, provider: str) -> Dict:
        return self.usage.get(provider, {"tokens": 0, "requests": 0, "cost": 0.0})


class LLMProvider(ABC):
    """LLM ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.name = self.__class__.__name__
        self.priority = 0  # ë‚®ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ
        self.last_error_time = None
        self.error_count = 0
        self.max_retries = 3
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        pass
    
    @abstractmethod
    def estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì •"""
        pass
    
    def can_retry(self) -> bool:
        """ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€"""
        if self.last_error_time:
            # 5ë¶„ ì´ë‚´ 3íšŒ ì´ìƒ ì˜¤ë¥˜ ì‹œ ì‚¬ìš© ì¤‘ë‹¨
            if (datetime.now() - self.last_error_time).seconds < 300:
                return self.error_count < self.max_retries
        return True
    
    def record_error(self):
        """ì˜¤ë¥˜ ê¸°ë¡"""
        self.error_count += 1
        self.last_error_time = datetime.now()
        
    def reset_errors(self):
        """ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹"""
        self.error_count = 0
        self.last_error_time = None


class GeminiProvider(LLMProvider):
    """Google Gemini ì œê³µì"""
    
    def __init__(self, api_key: str):
        super().__init__()
        if not genai:
            raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        self.priority = 0  # ìµœìš°ì„ 
        
    async def generate(self, prompt: str, **kwargs) -> str:
        import asyncio
        from google.api_core import exceptions
        
        try:
            # generate_content is synchronous, so we run it in a thread
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, self.model.generate_content, prompt)
            self.reset_errors()  # ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´í„° ë¦¬ì…‹
            return response.text
        except exceptions.ResourceExhausted as e:
            logger.error(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
            self.record_error()
            raise APIRateLimitError("Gemini", str(e))
        except Exception as e:
            logger.error(f"Gemini ìƒì„± ì˜¤ë¥˜: {e}")
            self.record_error()
            raise APIError("Gemini", str(e))
    
    def is_available(self) -> bool:
        return genai is not None and os.getenv("GEMINI_API_KEY") and self.can_retry()
    
    def estimate_tokens(self, text: str) -> int:
        # ëŒ€ëµì ì¸ ì¶”ì • (í‰ê·  4ì = 1í† í°)
        return len(text) // 4


class Gemma3Provider(LLMProvider):
    """Gemma3 ë¡œì»¬ ëª¨ë¸ ì œê³µì (Ollama ì‚¬ìš©)"""
    
    def __init__(self, model_name: str = "gemma3:latest"):
        super().__init__()
        if not ollama:
            raise ImportError("ollama íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ollama")
        
        self.model_name = model_name
        self.client = ollama.Client()
        self.priority = 3  # ë¡œì»¬ ëª¨ë¸ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„
        
    async def generate(self, prompt: str, **kwargs) -> str:
        import asyncio
        try:
            # ollama client.generate is synchronous, so run in executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.client.generate(model=self.model_name, prompt=prompt)
            )
            self.reset_errors()
            return response['response']
        except Exception as e:
            logger.error(f"Gemma3 ìƒì„± ì˜¤ë¥˜: {e}")
            self.record_error()
            raise APIError("Gemma3", str(e))
    
    def is_available(self) -> bool:
        if not ollama:
            return False
        
        try:
            # Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ê³  ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
            models = self.client.list()
            return any(self.model_name in model['name'] for model in models['models']) and self.can_retry()
        except:
            return False
    
    def estimate_tokens(self, text: str) -> int:
        return len(text) // 4


class OpenAIProvider(LLMProvider):
    """OpenAI GPT ì œê³µì"""
    
    def __init__(self, api_key: str, model: str = "gpt-4-turbo-preview"):
        super().__init__()
        if not openai:
            raise ImportError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.priority = 1  # Gemini ë‹¤ìŒ ìš°ì„ ìˆœìœ„
        
    async def generate(self, prompt: str, **kwargs) -> str:
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            self.reset_errors()
            return response.choices[0].message.content
        except openai.RateLimitError as e:
            logger.error(f"OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
            self.record_error()
            raise APIRateLimitError("OpenAI", str(e))
        except Exception as e:
            logger.error(f"OpenAI ìƒì„± ì˜¤ë¥˜: {e}")
            self.record_error()
            raise APIError("OpenAI", str(e))
    
    def is_available(self) -> bool:
        return openai is not None and os.getenv("OPENAI_API_KEY") and self.can_retry()
    
    def estimate_tokens(self, text: str) -> int:
        # tiktokenì„ ì‚¬ìš©í•œ ì •í™•í•œ ê³„ì‚°ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ê°„ë‹¨íˆ ì¶”ì •
        return len(text) // 4


class AnthropicProvider(LLMProvider):
    """Anthropic Claude ì œê³µì"""
    
    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229"):
        super().__init__()
        if not AsyncAnthropic:
            raise ImportError("anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.client = AsyncAnthropic(api_key=api_key)
        self.model = model
        self.priority = 2  # OpenAI ë‹¤ìŒ ìš°ì„ ìˆœìœ„
        
    async def generate(self, prompt: str, **kwargs) -> str:
        try:
            # Anthropic API í˜•ì‹ì— ë§ê²Œ ë³€í™˜
            max_tokens = kwargs.pop("max_tokens", 4096)
            
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            self.reset_errors()
            return response.content[0].text
        except Exception as e:
            if "rate_limit" in str(e).lower():
                logger.error(f"Anthropic API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
                self.record_error()
                raise APIRateLimitError("Anthropic", str(e))
            else:
                logger.error(f"Anthropic ìƒì„± ì˜¤ë¥˜: {e}")
                self.record_error()
                raise APIError("Anthropic", str(e))
    
    def is_available(self) -> bool:
        return AsyncAnthropic is not None and os.getenv("ANTHROPIC_API_KEY") and self.can_retry()
    
    def estimate_tokens(self, text: str) -> int:
        # Claudeì˜ í† í° ê³„ì‚° ë°©ì‹
        return len(text) // 4


class LLMManager:
    """ë©€í‹° í”„ë¡œë°”ì´ë” LLM ê´€ë¦¬ì - ìë™ í´ë°± ì§€ì›"""
    
    def __init__(self):
        self.providers: List[LLMProvider] = []
        self.usage_tracker = UsageTracker()
        self._initialize_providers()
        
    def _initialize_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í”„ë¡œë°”ì´ë” ì´ˆê¸°í™”"""
        
        # Gemini
        gemini_key = os.getenv("GEMINI_API_KEY")
        if gemini_key:
            try:
                self.providers.append(GeminiProvider(gemini_key))
                logger.info("âœ… Gemini í”„ë¡œë°”ì´ë” í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Gemini í”„ë¡œë°”ì´ë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            try:
                self.providers.append(OpenAIProvider(openai_key))
                logger.info("âœ… OpenAI í”„ë¡œë°”ì´ë” í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ OpenAI í”„ë¡œë°”ì´ë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Anthropic
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key:
            try:
                self.providers.append(AnthropicProvider(anthropic_key))
                logger.info("âœ… Anthropic í”„ë¡œë°”ì´ë” í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Anthropic í”„ë¡œë°”ì´ë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Ollama (ë¡œì»¬)
        try:
            provider = Gemma3Provider()
            if provider.is_available():
                self.providers.append(provider)
                logger.info("âœ… Ollama í”„ë¡œë°”ì´ë” í™œì„±í™”")
        except Exception as e:
            logger.debug(f"Ollama í”„ë¡œë°”ì´ë” ì‚¬ìš© ë¶ˆê°€: {e}")
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        self.providers.sort(key=lambda p: p.priority)
        
        if not self.providers:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        logger.info(f"ğŸ¤– ì´ {len(self.providers)}ê°œì˜ LLM í”„ë¡œë°”ì´ë” í™œì„±í™”ë¨")
    
    async def generate(self, prompt: str, task_type: TaskType = "general", **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± - ìë™ í´ë°± ì§€ì›"""
        
        last_error = None
        
        # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” í™•ì¸ ë° ë³€ê²½ ê°ì§€
        current_provider = None
        for provider in self.providers:
            if provider.is_available():
                current_provider = provider.__class__.__name__
                # í”„ë¡œë°”ì´ë”ê°€ ë³€ê²½ëœ ê²½ìš° ë¡œê·¸
                if current_provider != getattr(self, '_last_used_provider', None):
                    provider_model = ""
                    if hasattr(provider, 'model'):
                        provider_model = f" ({provider.model})"
                    logger.info(f"ğŸ”„ LLM í”„ë¡œë°”ì´ë” ë³€ê²½: {current_provider}{provider_model}")
                    self._last_used_provider = current_provider
                break
        
        for provider in self.providers:
            if not provider.is_available():
                continue
                
            provider_name = provider.__class__.__name__
            logger.debug(f"ğŸ”„ {provider_name} ì‹œë„ ì¤‘...")
            
            try:
                # í† í° ì¶”ì •
                estimated_tokens = provider.estimate_tokens(prompt)
                
                # ìƒì„± ì‹¤í–‰
                start_time = datetime.now()
                result = await provider.generate(prompt, **kwargs)
                elapsed = (datetime.now() - start_time).total_seconds()
                
                # ì‚¬ìš©ëŸ‰ ì¶”ì 
                self.usage_tracker.track(
                    provider_name,
                    estimated_tokens,
                    self._estimate_cost(provider_name, estimated_tokens)
                )
                
                logger.debug(f"âœ… {provider_name} ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                return result
                
            except APIRateLimitError as e:
                logger.warning(f"âš ï¸ {provider_name} í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
                last_error = e
                continue
                
            except APIError as e:
                logger.error(f"âŒ {provider_name} ì˜¤ë¥˜: {e}")
                last_error = e
                continue
                
            except Exception as e:
                logger.error(f"âŒ {provider_name} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
                last_error = e
                continue
        
        # ëª¨ë“  í”„ë¡œë°”ì´ë” ì‹¤íŒ¨
        error_msg = "ëª¨ë“  LLM í”„ë¡œë°”ì´ë”ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
        if last_error:
            error_msg += f": {last_error}"
        
        logger.error(f"ğŸ’¥ {error_msg}")
        
        # ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ë¶„ì„ ì—ì´ì „íŠ¸ë¥¼ ìœ„í•´)
        if task_type in ["sentiment", "summary"]:
            return json.dumps({
                "summary": "LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "score": 0.0,
                "error": error_msg
            })
        else:
            raise APIError("LLMManager", error_msg)
    
    def _estimate_cost(self, provider_name: str, tokens: int) -> float:
        """í† í°ë‹¹ ë¹„ìš© ì¶”ì • (USD)"""
        costs_per_1k = {
            "GeminiProvider": 0.0,  # ë¬´ë£Œ í‹°ì–´
            "OpenAIProvider": 0.03,  # GPT-4
            "AnthropicProvider": 0.015,  # Claude-3
            "Gemma3Provider": 0.0  # ë¡œì»¬
        }
        return (tokens / 1000) * costs_per_1k.get(provider_name, 0.0)
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        stats = {}
        for provider in self.providers:
            provider_name = provider.__class__.__name__
            usage = self.usage_tracker.get_usage(provider_name)
            stats[provider_name] = {
                "available": provider.is_available(),
                "priority": provider.priority,
                "error_count": provider.error_count,
                "usage": usage
            }
        return stats
    
    def get_available_providers(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” ëª©ë¡"""
        return [p.__class__.__name__ for p in self.providers if p.is_available()]
    
    def reset_all_errors(self):
        """ëª¨ë“  í”„ë¡œë°”ì´ë”ì˜ ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹"""
        for provider in self.providers:
            provider.reset_errors()
        logger.info("ğŸ”„ ëª¨ë“  í”„ë¡œë°”ì´ë” ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_manager = None

def get_llm_manager() -> LLMManager:
    """LLM ê´€ë¦¬ì ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager
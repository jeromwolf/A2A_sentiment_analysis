#!/usr/bin/env python3
"""Advanced Data Agent - ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³ ê¸‰ ì—ì´ì „íŠ¸"""

import os
import json
import httpx
from fastapi import FastAPI, HTTPException
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from dotenv import load_dotenv
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Advanced News Data Agent", version="1.0.0")

# API í‚¤ ì„¤ì •
FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY")

# íšŒì‚¬ëª… - í‹°ì»¤ ë§¤í•‘
TICKER_TO_COMPANY = {
    "AAPL": "Apple",
    "GOOGL": "Google",
    "MSFT": "Microsoft", 
    "AMZN": "Amazon",
    "META": "Meta",
    "TSLA": "Tesla",
    "NVDA": "NVIDIA",
    "AMD": "AMD",
    "INTC": "Intel",
    "NFLX": "Netflix"
}

@app.get("/health")
async def health_check():
    """ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "api_keys_configured": {
            "finnhub": bool(FINNHUB_API_KEY),
            "news_api": bool(NEWS_API_KEY),
            "alpha_vantage": bool(ALPHA_VANTAGE_API_KEY)
        }
    }

@app.post("/collect_news/{ticker}")
async def collect_news(ticker: str) -> Dict:
    """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
    logger.info(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {ticker}")
    
    try:
        company_name = TICKER_TO_COMPANY.get(ticker.upper(), ticker)
        all_news = []
        
        # Finnhub APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘
        if FINNHUB_API_KEY:
            finnhub_news = await collect_finnhub_news(ticker)
            all_news.extend(finnhub_news)
        
        # NewsAPIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë°±ì—…)
        if NEWS_API_KEY and len(all_news) < 5:
            newsapi_news = await collect_newsapi_news(company_name, ticker)
            all_news.extend(newsapi_news)
        
        # ëª¨ì˜ ë°ì´í„° (API í‚¤ê°€ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°)
        if len(all_news) < 3:
            logger.warning(f"ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ëª¨ì˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            mock_news = generate_mock_news(ticker, company_name)
            all_news.extend(mock_news)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_news = remove_duplicates(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x.get("published_date", ""), reverse=True)
        
        # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ë°˜í™˜
        final_news = sorted_news[:10]
        
        logger.info(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(final_news)}ê°œ í•­ëª©")
        
        return {
            "ticker": ticker,
            "count": len(final_news),
            "news": final_news,
            "sources": list(set(item.get("source", "Unknown") for item in final_news))
        }
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def collect_finnhub_news(ticker: str) -> List[Dict]:
    """Finnhub APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    news_items = []
    
    try:
        async with httpx.AsyncClient() as client:
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ìµœê·¼ 7ì¼)
            to_date = datetime.now()
            from_date = to_date - timedelta(days=7)
            
            response = await client.get(
                "https://finnhub.io/api/v1/company-news",
                params={
                    "symbol": ticker,
                    "from": from_date.strftime("%Y-%m-%d"),
                    "to": to_date.strftime("%Y-%m-%d"),
                    "token": FINNHUB_API_KEY
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                
                for item in data[:5]:  # ìµœëŒ€ 5ê°œ
                    news_items.append({
                        "title": item.get("headline", ""),
                        "content": item.get("summary", ""),
                        "url": item.get("url", ""),
                        "source": item.get("source", "Finnhub"),
                        "published_date": datetime.fromtimestamp(item.get("datetime", 0)).isoformat(),
                        "sentiment": "neutral",  # Finnhubì€ ê°ì • ë¶„ì„ ì œê³µ ì•ˆí•¨
                        "api_source": "finnhub"
                    })
                    
                logger.info(f"ğŸ“ˆ Finnhubì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                
    except Exception as e:
        logger.error(f"Finnhub API ì˜¤ë¥˜: {e}")
    
    return news_items

async def collect_newsapi_news(company_name: str, ticker: str) -> List[Dict]:
    """NewsAPIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    news_items = []
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://newsapi.org/v2/everything",
                params={
                    "q": f"{company_name} OR {ticker}",
                    "apiKey": NEWS_API_KEY,
                    "language": "en",
                    "sortBy": "publishedAt",
                    "pageSize": 5
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                
                for article in data.get("articles", []):
                    news_items.append({
                        "title": article.get("title", ""),
                        "content": article.get("description", "") or article.get("content", ""),
                        "url": article.get("url", ""),
                        "source": article.get("source", {}).get("name", "NewsAPI"),
                        "published_date": article.get("publishedAt", ""),
                        "sentiment": "neutral",
                        "api_source": "newsapi"
                    })
                    
                logger.info(f"ğŸ“° NewsAPIì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                
    except Exception as e:
        logger.error(f"NewsAPI ì˜¤ë¥˜: {e}")
    
    return news_items

def generate_mock_news(ticker: str, company_name: str) -> List[Dict]:
    """ëª¨ì˜ ë‰´ìŠ¤ ë°ì´í„° ìƒì„±"""
    now = datetime.now()
    
    mock_templates = [
        {
            "title": f"{company_name} Reports Strong Q4 Earnings, Beats Analyst Expectations",
            "content": f"{company_name} ({ticker}) announced fourth-quarter earnings that exceeded Wall Street expectations, driven by strong product sales and expanding market share. Revenue grew 15% year-over-year.",
            "sentiment": "positive"
        },
        {
            "title": f"{company_name} Announces Major AI Investment Initiative",
            "content": f"{company_name} unveiled plans to invest $10 billion in artificial intelligence infrastructure over the next three years, positioning itself at the forefront of the AI revolution.",
            "sentiment": "positive"
        },
        {
            "title": f"Analysts Remain Cautious on {company_name} Stock Amid Market Volatility",
            "content": f"Several Wall Street analysts have maintained their 'hold' ratings on {ticker}, citing concerns about global economic conditions and increasing competition in key markets.",
            "sentiment": "neutral"
        }
    ]
    
    news_items = []
    for i, template in enumerate(mock_templates):
        news_items.append({
            "title": template["title"],
            "content": template["content"],
            "url": f"https://example.com/news/{ticker}-{i}",
            "source": "Market Analysis",
            "published_date": (now - timedelta(hours=i*8)).isoformat(),
            "sentiment": template["sentiment"],
            "api_source": "mock"
        })
    
    return news_items

def remove_duplicates(news_list: List[Dict]) -> List[Dict]:
    """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
    seen_titles = set()
    unique_news = []
    
    for item in news_list:
        title = item.get("title", "").lower().strip()
        if title and title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(item)
    
    return unique_news

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8007)